{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Preprocess.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9SSMMlZsWzl"
      },
      "source": [
        "#Pre - processing goals:\n",
        "\n",
        "#1.Remove all company names showed in the article \n",
        "#2.Regular Expression/Normalization â€” lowercase the words, remove punctuation and remove numbers\n",
        "#3.Stemming and lemmatization\n",
        "#4.Remove stop words\n",
        "#5.Tokenization\n",
        " \n"
      ],
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TuWmCYkevi59"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fEdFsSGfLx3a",
        "outputId": "e258e149-d1e0-4185-82dd-e098f57ef07a"
      },
      "source": [
        "#preprocessing libraries for nlp\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem.porter import PorterStemmer"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jar640EbkOOW"
      },
      "source": [
        "# We will use this for identifying company names and their spans\n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mX73KVZNvu2B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9afbdbf5-a26e-4d89-c327-795934bcf156"
      },
      "source": [
        "df = pd.read_csv('/content/drive/MyDrive/sharmadata.csv')\n",
        "df.columns\n"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['News_content', 'Link', 'Title'], dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GlHo04Csv7I5",
        "outputId": "cda1df9a-cf1d-401e-f60f-857a370935f1"
      },
      "source": [
        "#checking of our all article DataFrame contains any NUll values\n",
        "df.isnull().sum()"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "News_content    0\n",
              "Link            0\n",
              "Title           0\n",
              "dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhixBLz_xMvM"
      },
      "source": [
        "wordnet = WordNetLemmatizer()\n",
        "ps      = PorterStemmer()"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKw36I64jU9F"
      },
      "source": [
        "#Removing Company names using NER model in spacy\n",
        "\n",
        "def Remove_company_names(article):\n",
        "  doc  = nlp(article)\n",
        "  aft_comp_remov_art = article # we will save this functions output in this variable \n",
        "  for ent in doc.ents:\n",
        "    if ent.label_ == 'ORG' :\n",
        "      aft_comp_remov_art = re.sub(ent.text, ' ', aft_comp_remov_art)\n",
        "\n",
        "  return aft_comp_remov_art #Returning a string having company names removed\n"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "udtB-LE0wS7b"
      },
      "source": [
        "def All_other_pre_processing(article):\n",
        "  corpus = []\n",
        "  review = re.sub('[^a-zA-Z]', ' ', article) #Remove everything(Punctuations, Numbers, etc... except the alphabetical words)\n",
        "  review = review.lower()                     #lowercasing the words\n",
        "  review = review.split()\n",
        "  review = [wordnet.lemmatize(word) for word in review if not word in stopwords.words('english')] #removing stopwords and lemmatizing\n",
        "  review = [ps.stem(word) for word in review] #stemming\n",
        "  review = ' '.join(review)\n",
        "  corpus.append(review)\n",
        "\n",
        "  return corpus[0] #Returning final preprocessed article string"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoYYO6TxrO49"
      },
      "source": [
        "def tokenizing(article):\n",
        "#tokenization\n",
        "#we can also use tokenizers library\n",
        "  tokens = [word for word in article.split()]\n",
        "  return tokens"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHXlQDXOm9G1"
      },
      "source": [
        "un_processed_article = df['News_content'][1]\n",
        "\n",
        "comp_names_remov_article = Remove_company_names(un_processed_article)\n",
        "\n",
        "final_article = All_other_pre_processing(comp_names_remov_article)\n",
        "\n",
        "final_article_tokens = tokenizing(final_article)"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-xWGsZhIqbSz",
        "outputId": "95990298-8e67-427a-d7c1-c67681158db3"
      },
      "source": [
        "print(len(un_processed_article))\n",
        "print(len(comp_names_remov_article))\n",
        "print(len(final_article))"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8075\n",
            "7823\n",
            "4215\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8yfqgsAyKBl"
      },
      "source": [
        "un_processed_article"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7CpSHFByQZ8"
      },
      "source": [
        "final_article"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "poFB85UZz2F-"
      },
      "source": [
        "final_article_tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBVakkyWPIbU"
      },
      "source": [
        "#confirming if there is a number in our article!?\n",
        "pattern = re.compile(r'\\d+')\n",
        "matches = pattern.finditer(final_article)\n",
        "for match in matches:\n",
        "  print(match)"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JYF_IH-Xxl23",
        "outputId": "2475d78f-1e0d-43af-cd88-5b47717ed88c"
      },
      "source": [
        "#Checking if we removed company names\n",
        "doc  = nlp(comp_names_remov_article )  \n",
        "for ent in doc.ents:\n",
        "  if ent.label_ == 'ORG' :\n",
        "    print(ent.text)"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V7Ud8YhRYAbD"
      },
      "source": [
        "#Method 2 to remove company names without using re\n",
        "#Here We are using Named entity recognition function of spacy\n",
        "\n",
        "def Method2_comp_name_remov(article):\n",
        "  doc  = nlp(article)\n",
        "  start = 0\n",
        "  aft_remov_comp = '' #Here we will store the final article\n",
        "  for ent in doc.ents:\n",
        "    if ent.label_ == 'ORG' :\n",
        "      aft_remov_comp = aft_remov_comp + article[start:ent.start_char]  \n",
        "      start = ent.end_char+1\n",
        "\n",
        "  aft_remov_comp = aft_remov_comp + text[start:len(text)]\n",
        "\n",
        "  return aft_remov_comp\n"
      ],
      "execution_count": 77,
      "outputs": []
    }
  ]
}